{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"How can we automatically tag each word of a text with its word class?\"\n",
    "tokenized = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    0    1    2              3    4     5     6   7   8     9     10    11  \\\n0  How  can   we  automatically  tag  each  word  of   a  text  with   its   \n1  WRB   MD  PRP             RB  VBP    DT    NN  IN  DT    NN    IN  PRP$   \n\n     12     13 14  \n0  word  class  ?  \n1    NN     NN  .  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How</td>\n      <td>can</td>\n      <td>we</td>\n      <td>automatically</td>\n      <td>tag</td>\n      <td>each</td>\n      <td>word</td>\n      <td>of</td>\n      <td>a</td>\n      <td>text</td>\n      <td>with</td>\n      <td>its</td>\n      <td>word</td>\n      <td>class</td>\n      <td>?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WRB</td>\n      <td>MD</td>\n      <td>PRP</td>\n      <td>RB</td>\n      <td>VBP</td>\n      <td>DT</td>\n      <td>NN</td>\n      <td>IN</td>\n      <td>DT</td>\n      <td>NN</td>\n      <td>IN</td>\n      <td>PRP$</td>\n      <td>NN</td>\n      <td>NN</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag = nltk.pos_tag(tokenized)\n",
    "pd.DataFrame(pos_tag).T  # the t is to transpose it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset('NN')  # this is just for getting the info what the shortcuts stand for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      0       1   2       3    4   5       6    7       8       9\n0  They  refuse  to  permit   us  to  obtain  the  refuse  permit\n1   PRP     VBP  TO      VB  PRP  TO      VB   DT      NN      NN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>They</td>\n      <td>refuse</td>\n      <td>to</td>\n      <td>permit</td>\n      <td>us</td>\n      <td>to</td>\n      <td>obtain</td>\n      <td>the</td>\n      <td>refuse</td>\n      <td>permit</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PRP</td>\n      <td>VBP</td>\n      <td>TO</td>\n      <td>VB</td>\n      <td>PRP</td>\n      <td>TO</td>\n      <td>VB</td>\n      <td>DT</td>\n      <td>NN</td>\n      <td>NN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"They refuse to permit us to obtain the refuse permit\"\n",
    "test1 = nltk.pos_tag(text.split())  # instead of the other thing (this is only one line though)\n",
    "pd.DataFrame(test1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## retrieving similar text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.gutenberg.words('melville-moby_dick.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man king hand whale devil as bird mast job lord day serpent boat stone\n",
      "ship harris captain mariner fellow hands\n"
     ]
    }
   ],
   "source": [
    "text.similar('woman')  # not similar to the meaning but similar like also a noun\n",
    "# text.similar('bought')\n",
    "# text.similar('over')\n",
    "# text.similar('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"I saw a yellow bird flying into my room through my clear window.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunk_grammar = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "parser = nltk.RegexpParser(chunk_grammar)\n",
    "tree = parser.parse(tags)\n",
    "# print(tree)\n",
    "# tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let's try again using gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence2 = \"The little yellow dog barked at the angry cat that belongs to Heidi Choi.\"\n",
    "tokens = nltk.word_tokenize(sentence2)\n",
    "tags = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pattern = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>}\n",
    "VBD: {<VBD>}\n",
    "IN: {<IN>}\n",
    "NP: {<NNP>+}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NPchunker = nltk.RegexpParser(pattern)\n",
    "result = NPchunker.parse(tags)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "hamlet_sentences = default_st(text=hamlet)\n",
    "hamlet_sentences = [x.replace('\\n', '') for x in hamlet_sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [],
   "source": [
    "hamlet_word = nltk.word_tokenize(hamlet)\n",
    "hamlet_pos = nltk.pos_tag(hamlet_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for word, pos in hamlet_pos:\n",
    "    if pos in ['NN']:\n",
    "        nouns.append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = ['pythoned', 'pythoning', 'pythons']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_text = \"it is very important to be pythonly while you are pythoning with python. All pythoners have pythoned at least once\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "# for w in words:\n",
    "# print(ps.stem(w)) this does not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's try snowball stemmer..better than Porterstemmer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'player', 'pharmaci', 'bad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "words = ['plays', 'playing', 'played', 'player', 'pharmacies', 'badly']\n",
    "ss = SnowballStemmer(language='english')\n",
    "print([ss.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lancaster Stemmer? - A little aggressive.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'play', 'pharm', 'bad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "print([ls.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Might not bring you the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'playing', 'played', 'player', 'pharmacy', 'badly']\n"
     ]
    }
   ],
   "source": [
    "print([wnl.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What happened here? pharmacy has been fixed but what about others?! <br>\n",
    "Remember! Without the POS tag, wnl assume every word as a noun! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "go\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('dogs', 'n'))\n",
    "print(wnl.lemmatize('went', 'v'))\n",
    "print(wnl.lemmatize('happiest', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "There were doors all round the hall, but they were all locked; \n",
    "and when Alice had been all the way down one side and up the other, trying every door, \n",
    "she walked sadly down the middle, wondering how she was ever to get out again.. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence_words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punt = \"?!:;,.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for word in sentence_words:\n",
    "    if (word in punt):\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "# maybe try it in the other way for for loops\n",
    "# sentence_words = (sentence_words.remove(word) for word in sentence_words if word in punt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There     There     \n",
      "were      be        \n",
      "doors     doors     \n",
      "all       all       \n",
      "round     round     \n",
      "the       the       \n",
      "hall      hall      \n",
      "but       but       \n",
      "they      they      \n",
      "were      be        \n",
      "all       all       \n",
      "locked    lock      \n",
      "and       and       \n",
      "when      when      \n",
      "Alice     Alice     \n",
      "had       have      \n",
      "been      be        \n",
      "all       all       \n",
      "the       the       \n",
      "way       way       \n",
      "down      down      \n",
      "one       one       \n",
      "side      side      \n",
      "and       and       \n",
      "up        up        \n",
      "the       the       \n",
      "other     other     \n",
      "trying    try       \n",
      "every     every     \n",
      "door      door      \n",
      "she       she       \n",
      "walked    walk      \n",
      "sadly     sadly     \n",
      "down      down      \n",
      "the       the       \n",
      "middle    middle    \n",
      "wondering wonder    \n",
      "how       how       \n",
      "she       she       \n",
      "was       be        \n",
      "ever      ever      \n",
      "to        to        \n",
      "get       get       \n",
      "out       out       \n",
      "again     again     \n",
      "..        ..        \n",
      "There     There     \n",
      "were      were      \n",
      "doors     door      \n",
      "all       all       \n",
      "round     round     \n",
      "the       the       \n",
      "hall      hall      \n",
      "but       but       \n",
      "they      they      \n",
      "were      were      \n",
      "all       all       \n",
      "locked    locked    \n",
      "and       and       \n",
      "when      when      \n",
      "Alice     Alice     \n",
      "had       had       \n",
      "been      been      \n",
      "all       all       \n",
      "the       the       \n",
      "way       way       \n",
      "down      down      \n",
      "one       one       \n",
      "side      side      \n",
      "and       and       \n",
      "up        up        \n",
      "the       the       \n",
      "other     other     \n",
      "trying    trying    \n",
      "every     every     \n",
      "door      door      \n",
      "she       she       \n",
      "walked    walked    \n",
      "sadly     sadly     \n",
      "down      down      \n",
      "the       the       \n",
      "middle    middle    \n",
      "wondering wondering \n",
      "how       how       \n",
      "she       she       \n",
      "was       wa        \n",
      "ever      ever      \n",
      "to        to        \n",
      "get       get       \n",
      "out       out       \n",
      "again     again     \n",
      "..        ..        \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print(\"{0:10}{1:10}\".format(word, wnl.lemmatize(word,\n",
    "                                                    pos='v')))  # this will not be on the exam (we just did it to view the stuff in there!\n",
    "\n",
    "for word in sentence_words:\n",
    "    print(\"{0:10}{1:10}\".format(word, wnl.lemmatize(word,\n",
    "                                                    pos='n')))  # this will not be on the exam (we just did it to view the stuff in there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}