{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MidTerm Summary 1-5\n",
    "## Scrape data from website\n",
    "scrape authors and quotes from this website: http://quotes.toscrape.com/ (all 10 pages).\n",
    "combine them in a Dataframe and export that Dataframe to Excel\n",
    "- (Then try the same thing with dictionaries)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "response = requests.get(\"http://quotes.toscrape.com\")\n",
    "\n",
    "quotes = []\n",
    "authors = []\n",
    "for pageNr in range(1, 11):\n",
    "    response = requests.get(f\"http://quotes.toscrape.com/page/{pageNr}/\")\n",
    "    responseText = bs(response.text, \"html.parser\")\n",
    "    quotes_tags = responseText.find_all('span', class_='text')\n",
    "    authors_tags = responseText.find_all('small', class_='author')\n",
    "    for quote_tag in quotes_tags:\n",
    "        quotes.append(quote_tag.get_text())\n",
    "    for authors_tag in authors_tags:\n",
    "        authors.append(authors_tag.get_text())\n",
    "\n",
    "quotesDF = pd.DataFrame(data=quotes, columns=[\"Quotes\"])\n",
    "authorsDF = pd.DataFrame(data=authors, columns=[\"Authors\"])\n",
    "combinedDF = pd.concat([quotesDF, authorsDF], axis=1)\n",
    "# combinedDF.to_excel(\"combined_authors_quotes.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Try the same thing with dictionaries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    " get the data from \"https://www.gutenberg.org/cache/epub/8001/pg8001.html\",\n",
    " - remove newline and \\r, and everything that is not a word\n",
    " - put the words into lowercase\n",
    " - tokenize the text into words\n",
    " - find the 10 words that occur the most"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.gutenberg.org/cache/epub/8001/pg8001.html\")\n",
    "responseText = bs(response.text, \"html.parser\")\n",
    "paragraphs = responseText.find_all(\"p\")\n",
    "paragraphs_not_all = []\n",
    "for paragraph in paragraphs[0:50]:\n",
    "    paragraphs_not_all.append(\" \".join(w.lower() for w in paragraph.get_text().split()))\n",
    "\n",
    "import re\n",
    "\n",
    "words = []\n",
    "for p in paragraphs_not_all:\n",
    "    for w in re.sub(r'[^A-Za-z]+', ' ', p).split():\n",
    "        words.append(w)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(words)\n",
    "# counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " get the Alice in Wonderland text from the Gutenberg Corpus\n",
    " - remove newlines\n",
    " - remove everything that is not a word\n",
    " - lowercase\n",
    " - find the 10 words that occur the most\n",
    " - remove the stopwords and find the 10 words that occur the most again"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "alice_raw = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "alice_raw_words = re.sub(r'[^A-Za-z]+', ' ', alice_raw)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "alice_filtered = [word.lower() for word in alice_raw_words.split() if word not in stop_words]\n",
    "counter = Counter(alice_filtered)\n",
    "# counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## POS-tagging\n",
    " download nltk tagsets and POS-tag sentence1 and then pos-tag sentence2\n",
    " and chunk to these patterns\n",
    " - NP: {<DT>?<JJ>*<NN>}\n",
    " - VBD: {<VBD>}\n",
    " - IN: {<IN>}\n",
    " - NP: {<NNP>+}\n",
    " and then show the results in a tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading tagset: Package 'tagset' not found in index\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"tagset\")\n",
    "sentence1 = \"They refuse to permit us to obtain the refuse permit\"\n",
    "sentence2 = \"The little yellow dog barked at the angry cat that belongs to Heidi Choi.\"\n",
    "\n",
    "sentence1_pos_DF = pd.DataFrame(nltk.pos_tag(sentence1.split()))\n",
    "sentence2_pos = nltk.pos_tag(sentence2.split())\n",
    "# sentence1_pos.T\n",
    "\n",
    "pattern = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>}\n",
    "VBD: {<VBD>}\n",
    "IN: {<IN>}\n",
    "NP: {<NNP>+}\n",
    "\"\"\"\n",
    "\n",
    "NPchunker = nltk.RegexpParser(pattern)  # -----------------!-----------------\n",
    "result = NPchunker.parse(sentence2_pos)  # ----------------!-----------------\n",
    "# result.draw()  # ----------------------------------------!-----------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " download the hamlet text and pos-tag it and then through that get all the nouns out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()\n",
    "hamlet = gutenberg.raw(fileids='shakespeare-hamlet.txt')\n",
    "hamlet_pos = nltk.pos_tag(hamlet.split())\n",
    "nouns = []\n",
    "for word, pos in hamlet_pos:\n",
    "    if pos in ['NN']:\n",
    "        nouns.append(word)\n",
    "\n",
    "#print(nouns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "words = ['plays', 'playing', 'played', 'player', 'pharmacies', 'badly']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " These are the given words. Use the 3 stemmers to get the base of the words\n",
    " Then lemmatize these words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'player', 'pharmaci', 'badli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "print([porter_stemmer.stem(word) for word in words])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'player', 'pharmaci', 'bad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "print([snowball_stemmer.stem(word) for word in words])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'play', 'pharm', 'bad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print([lancaster_stemmer.stem(word) for word in words])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# print([lemmatizer.lemmatize(w) for w in words])\n",
    "\n",
    "words_tagged = nltk.pos_tag(words)\n",
    "# for word, pos in words_tagged:\n",
    "#     print(lemmatizer.lemmatize(word=word, pos=pos))\n",
    "# -------------!----------------!--------------!----------------\n",
    "# I think the tags might be different and that is the reason why this is not working\n",
    "# ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Googlenews\n",
    "- get the top news\n",
    "- get the top business articles\n",
    "- get everything in NewYork in 2022 (%22New+York%22) and print the values for each key and the title of every entry\n",
    "- put every title and link from every story from search entry weather in a Dataframe\n",
    "- Remove a column in the Dataframe\n",
    "- Rename a column in the Dataframe\n",
    "- Reorder the Dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "\n",
    "gn = GoogleNews()\n",
    "top_news = gn.top_news()\n",
    "business = gn.topic_headlines('business')\n",
    "new_york = gn.search('%New+York%22')\n",
    "# new_york.keys() # -> dict_keys(['feed', 'entries'])\n",
    "\n",
    "# for entry in new_york['entries']:\n",
    "# print(entry.title) # or print(entry['title'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "weather = gn.search('weather')\n",
    "weather_stories = []\n",
    "for story in weather['entries']:\n",
    "    weather_stories.append({'title': story['title'], 'link': story['link']})\n",
    "\n",
    "dataframe = pd.DataFrame(weather_stories)\n",
    "dataframe['Title'] = dataframe['title'].str.split('-', expand=True)[0]\n",
    "dataframe['Source'] = dataframe['title'].str.split('-', expand=True)[1]\n",
    "dataframe.drop(['title'], axis=1, inplace=True)\n",
    "dataframe.rename(columns={'link': \"Link\"}, inplace=True)\n",
    "dataframe = dataframe[[\"Title\", \"Source\", \"Link\"]]\n",
    "# dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}